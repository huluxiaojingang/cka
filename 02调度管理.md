# 大纲要求
- Use label selectors to schedule Pods
- Understand the role of DaemonSets
- Understand how resource limits can affect Pod scheduling
- Understand how to run multiple schedulers and how to configure Pods to use them
- Manualy schedule a pod without a scheduler
- Display scheduler events
- Know how to configure the Kubernetes scheduler.

# scheduling

为Pod选择合适的node

```
kubectl get node <node-name> -o yaml  查看一个完整的node
```

Node对象status.allocatable：一个node可分配资源量

# Pod中影响调度的主要属性字段
- spec.containers.resources.requests 资源调度依据，cpu、memory。
- spec.containers.resources.limits 限制Pod的使用资源商险，不影响调度
- spec.schedulerName: default-scheduler 调度器。多调度器时不同的调度器负责不同Pod时会用到
- spec.nodeName: 调度结果

高级调度策略

- spec.nodeSelector: {...}
- spec.affinity: {...}
- spec.tolerations: {...}

# 调度器的资源分配机制

#### 基于Pod中容器request资源“总和”调度
- 未指定request资源时，按0资源需求进行调度
- initContainer取最大值，container取累加值，最后取大，Max(Max(initContainers.requests),Sum(containers.requests))。initContainer，逐个执行，拉起后退出。

```
...
spec:
  initContainers:
  - name: ic1
    resources:
      requests:
        cpu: "1"
        memory: "1G"
  - name: ic2
    resources:
      requests:
        cpu: "1"
        memory: "3G"
  containers:
  - name: container1
    resources:
      requests:
        cpu: "500m"
        memory: "1G"
  - name: container2
    resources:
      requests:
        cpu: "500m"
        memory: "1G"
...
```

最终结果：cpu 1 ， memory 3G



#### 基于资源声明量的调度，而非实际占用
- 不依赖监控，系统不会过去敏感
- 能否调度成功：pod.request < node.allocatable - node.requested
- 资源模型：node资源-系统预留-k8s组件预留-硬驱逐-node allocatable resources

#### 资源分配算法
- GeneralPerdicates （主要是PodFitsResources）
- LeastRequestedPriority 平衡节点被调度器操作次数，最少被调度Pod的节点优先选中
- BalancedResourceAllocation，平衡cpu/memory的消耗比例，计算Pod的cpu/memory比例和Node上cpu/memory比例，避免Pod上memory不足cpu剩余，或者cpu不足内存剩余，提高node资源利用率

# 高级调度策略
## nodeSelector
  比较简单，语法功能简单
- 匹配node.labels。node必须匹配nodeSelector中的key-value，node中包含其他标签不影响，但是不能少标签
- 排除不包含nodeSelector中指定label的所有node
- 匹配机制--完全匹配

 ```
 kind: Pod
 spec:
   containers:
   ...
   nodeSelector:
     disktype: ssd
     node-flavor: s3.large.2
  ```

## nodeAffinity
  nodeSelector的升级版，语法更复杂、功能更强大，与nodeSelector关键差异：
  - 引入运算符：In，NotIn (labelselector语法)
  - 支持枚举label可能的取值，如 zone in [az1,az2,az3...]
  - 支持硬性过滤和软性评分
  - 硬性过滤规则支持执行多条件之间的逻辑或运算
  - 软性评分规则支持设置条件权重值
  
  ```
  ...
  kind: Pod
  spec:
    affinity:
      nodeAffinity:
        # 必须满足的条件
        requiredDuringSchedulingIgnoredDuringExecution:
          # 多个nodeSelectorTerms之前是or
          nodeSelectorTerms:
          - matchExpressions:
            # 多个matchExpressions之前是and
            - key: node-flavor
              operator: In
              values:
              - s3.large.2
              - s3.large.3
        # 优先满足的条件，如果不满足，也调度成功
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
            - key: node-flavor
              operator: In
              values:
              - s3.large.2
  ...
  ```

## podAffinity
  让多个pod部署到同一个node或者同一个区域或者同一个机架上，减少时延，节约流量。一些公有云的流量可能是在区域内是免费的或者低收费，夸区域是收费的。可以做到podAffinity与自身匹配，即多个Pod部署到同一个区域内。与nodeAffinity的关键差异
- 定义在PodSoec中，亲和与反亲和规则具有对称性
- labelSelector的匹配对象为Pod
- 对node分组，依据lable-key=topologyKey,每个label-value取值为一组
- 硬性过滤规则，条件之间只有逻辑与运算

```
kind: Pod
spec:
  affinity
    podAffinity:
      # 本Pod必须和拥有label security=S1的Pod调度到topologykey指定的级别在一起，topologykey可以是同一个节点、机架、或自定义的分组
      # node分组通过topologykey指定
      # 必备条件
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
        matchExpressions:
        # 多个条件之前是 and
        - key: security
          operator: In
          values:
          - S1
        topologykey: kubernetes.io/zone
      # 优先条件
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
  containers:
  ...
```


## podAntiAffinity
  反亲和，避免某些Pod分组在同一组Node上，与podAffinity的差异
- 匹配过程相反，即podAffinity中可调度的节点，在podAntiAffinity中为不可调度
- 最终处理调度结果时取反，即podAffinity中的高分节点，在podAntiAffinity中为低分

```
kind: Pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
        matchExpressions:
        - key: security
          operator: In
          values:
          - S1
        topologykey: kubernetes.io/zone
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: kubernetes.io/hostname
```

## 手动调度Pod
  适应场景：调度器不工作，临时救急。封装实现自定义调度器。创建Pod时候直接指定spec.nodeName

```
kind: Pod
spec:
  containers:
  ...
  nodeName: work-1
```

## DaemonSet
  每个节点创建一个相同的pod。通常用来部署集群中的agent，如网络插件
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      name: my-daemonset
  template:
    metadata:
      labels:
        name: my-daemonset
    spec:
      containers:
      - name: container
        image: k8s.gcr.io/pause:2.0
```
等价于：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deploy
spec:
  replicas: {number of nodes}
  selector:
    matchLabels:
      podlabel: daemonset
  template:
    metadata:
      labels:
        podlabel: daemonset
    spec:
      containers:
      - name: container
        image: k8s.gcr.io/pause:2.0
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
            matchExpressions:
              - key: podlabel
                operator: In
                values:
                - daemonset
            topologyKey: kubernetes.io/hostname
```
我的Pod有label  podlabel: daemonset，我不要调度到节点上的Pod有label podlabel: daemonset的节点，即每个节点部署一个pod。
replicas是变量不方便使用，所以有DaemonSet对象


## Taints
  避免Pod调度到特定Node上，预留特殊节点做特殊用途。taints是Node的属性
- 带effect的特殊label，对Pod有排斥性。硬性排斥：NoSchedule；软性排斥：PreferNoSchedule
- 系统创建的taint附带时间戳 , effect 为NoSchedule，便于触发对Pod的超时驱逐。
  
```
kind: Node
spec:
  taints:
  - effect: NoSchedule
    key: accelerator
    timeAdded: null
    value: gpu
```

taint命令和label类似，添加taint命令
```
kubectl taint node node-1 foo=bar:NoSchedule
```

删除taint
```
kubectl taint node node-1 fool:NoSchedule-
```

##  Tolerations
  允许Pod调度到有特定taints的Node上
  
如下node有taints，标记为此node有gpu资源。
```
kind: Node
spec:
  externalID: node-1
  taints:
  - effect: NoSchedule
    key: accelerator
    timeAdded: null
    value: gpu
```
如下pod可以无视排斥，有gpu需求的pod配置tolerations，可以调度到有gpu资源的node上
```
kind: Pod
spec:
  containers:
  ...
  tolerations:
  - key: accelerator
    # Equal 完全匹配  <key>=<value>:<effect>
    # Exists 匹配任意taint value  如 <key>:<effect>
    operator: Equal
    value: gpu
    effect: NoSchedule
```

node上如果有多个taints，pod的tolerations必须匹配所有才能被调度上。这里和nodeSelector不一样，nodeSelector是匹配任意一个即可被调度上。

## 调度失败原因分析

```
kubectl get pod [podname] -o wide
kubectl describe po [podname]
```

## 多调度器
  适用场景：集群中存在多个调度器，分别处理不同类型的作业调度。建议对node做资源池划分，避免调度结果写入冲突

```
kind: Pod
spec:
  containers:
  ...
  schedulerName: my-custom-scheduler
```

# 课堂练习

## 手动调度
  直接指定nodeName
```
kind: Pod
spec:
  containers:
  ...
  nodeName: "xxxxx"
```

## nodeAffinity

  首先给node打上label
```
kubectl label node  worker1.vagrant.vm has-eip=yes
```

  声明Pod的yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity
  labels:
    run: node-affinity
spec:
  containers:
  - name: node-affinity
    image: nginx
    imagePullPolicy: IfNotPresent
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: has-eip
            operator: In
            values:
            - "yes"
```

## podAffinity
  此Pod会和有label run=node-affinity的pod调度到同一个node上。topologyKey: kubernetes.io/hostname 表示node的hostname相，意思就是同一个node。
```
apiVersion: v1
kind: Pod
metadata:
  name: pod-affinity
  labels:
    run: pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: run
            operator: In
            values:
            - "node-affinity"
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: nginx
```

## podAntiAffinity
  此pod与目标pod在hostname级别做反亲和，即我不要和你调度到同一个node上
```
apiVersion: v1
kind: Pod
metadata:
  name: pod-anti-affinity
  labels:
    run: pod-anti-affinity
spec:
  containers:
  - name: pod-affinity
    image: nginx
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key : run
            operator: In
            values:
            - "pod-affinity"
        topologyKey: kubernetes.io/hostname
```















