# Instalation,Configuration &Validation 12%

# install master

# 集群部署

## Master节点
- etct
- kube-apiserver
- kube-controller-manager
- kube-scheduler
- ...

开始尝试virtualBox Ubuntu虚拟机部署，但是遇到一些奇怪的问题，最终放弃了采用阿里云两台ECS机器。

  
1、install docker
```
apt-get update
apt-get install -y docker.io
```

2、install kubelet kubeadm kubelet
```
关闭swap：vi /etc/fstab 注释掉/swap  重启系统

apt-get update && apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

3、kubeadm init
```
# kubeadm init 以Pod形式拉起 master 组件。
kubeadm init --pod-network-cidr=10.244.0.0/16 #pod分配的ip网段

# master 安装成功后需要，执行以下命令
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Static Pod
```
kubelet拉起 Static Pod 启动master组件。当kubelet启动时，kubelet --pod-manifest-path=<the directory>，自动创建目录下的所有pod
kubelet 进程 --config参数指定了启动参数，找到staticPodPath字段，
kubelet 会watch配置目录文件的变化，并作出响应。

如何启动Static Pod？ 找到kubelet --pod-manifest-path目录，把自己写的pod yaml文件丢在这个目录下，kubectl会自动创建Static Pod
```




## Node节点
- kubelet
- kube-proxy
- docker
- CNI 插件


同master节点一样安装 kubelet kubeadm kubelet

在master执行，查看kubeadm token
```
kubeadm token list
```

添加node节点命令
```
  kubeadm join 172.24.199.161:6443 --token p0kwhh.kg02s86ge9a0xxk2 --discovery-token-ca-cert-hash  sha256:82d04a08da055d5b8c4da0ebef112c01082081cd10d9ac4e377d187af109b5ec

```
master 节点执行查看node，可以发现已经添加了新的node，但是状态是NotReady
```
kubectl get nodes
```

安装CNI插件，这里用flannel，从官网找地址
```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml
```
master节点执行。会创建DaemonSet对象。意味着node也会自动安装。Daemonset安装成功后，集群正常部署完成。  可以执行  kubectl get nodes 显示节点状态是Ready。
kubeconfig相关概念clusters、users、contexts，多集群安全配置参考官网文档
https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

# 安全配置

## 集群组件如何交互

  apiserver与etcd之间会生成etcd相关证书，配置给apiserver用来访问etcd。
  
  kube-proxy、scheduler、kubelet、controller-manager通过kubeconfig文件，提供client认证凭据来访问apiserver.
  
  kubectl config view 查看kubeconfig
```
apiVersion: v1
clusters: # 集群信息，访问方式，CA等
- cluster:
    certificate-authority-data: REDACTED
    server: https://192.168.99.100:6443
  name: kubernetes
contexts: # cluster、user，
- context: # 集群和客户端认证方式映射关系
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users: # 客户端认证凭据，token，password，证书等
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```
## 生成kubeconfig文件
  kubectl config -h 获取帮助文档
  
  修改cluster
```
kubectl config set-cluster kubernetes
      --certificate-authority=/path/ti/ca
      --embed-certs=true
      --server=${KUBE_APISERVER}
      --kubeconfig=/kubeconfig/filename
```

  修改user
```
kubectl config set-credentials testuser
      --client-certificate=/path/to/cert
      --client-key=/path/to/private_key
      --embed-certs=true
      --kubeconfig/kubeconfig/filename
``` 

  修改context
```
kubectl config set-context default
  --cluster=
  --user=
  --namespace=
  --kubeconfig=test.kubeconfig
```
  
  设置默认context
```
kubectl config use-context default --kubeconfig=/path/to/filename
```
  生成kubeconfig文件
```
#!/bin/bash

export KUBE_APISERVER="https://172.24.92.189:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.crt \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=test.kubeconfig
  
# --kubeconfig=test.kubeconfig 已经存在则新增，没有则创建
# --embed-certs=true 会把CA内容写进去，不加的话certificate-authority存的是证书路径
  
kubectl config set-credentials testuser \
  --client-certificate=/etc/kubernetes/pki/apiserver.crt \
  --client-key=/etc/kubernetes/pki/apiserver.key \
  --embed-certs=true \
  --kubeconfig=test.kubeconfig
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=testuser \
  --kubeconfig=test.kubeconfig
  
kubectl config use-context default --kubeconfig=test.kubeconfig

# 使用test.kubeconfig访问apiserver
kubectl get pods --kubeconfig=./test.kubeconfig

```
  kubeconfig文件是kube-proxy、scheduler、kubelet、controller-manager组件访问kube-apiserver用的


## TLS bootstrap与节点证书签发
  一个新的node节点加入k8s集群时，给节点签发客户端证书，节点名作为kubelet访问apiserver的用户名。不同的节点用户名不一样。当集群规模越来越大，证书签发任务太繁重。所以需要一种自动证书签发。
  
###   自动证书签发流程
 - kubelet启动时使用低权限token向kube-apiserver发送csr请求。kubeadm join --token 里的token。rbac配置权限
 - controller-mmanager会自动把证书签发下来给kube-apiserver。kubelet等待apiserver，对应的CRS资源对象在status字段里会写入证书。
 - kubelet拿到证书后写到节点证书路径下。
 - kubelet用新的证书访问apiserver

### RBAC允许的CRS请求类型

   kubelet第一次启动后发送签发证书请求给apiserver，鉴权会绑定ClusterRole，有以下三类：
- nodeclient：签发证书
- selfnodeclient：更新证书
- selfnodeserver：更新kubelet server 证书

kubelet启动参数
```
# 这个配置里有token，就是kubeadm join --token对应的token
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf

--kubeconfig=/etc/kubernetes/kubelet.conf
```
#### secret

apiserver
```
#  启动参数
--enable-bootstrap-token-auth=true ,在kube-system下创建Secret，bootstrap-token=xxx用于鉴权。

# kubectl get secret -n kube-system | grep boot
  bootstrap-token-xxxxx
  
kind: Secret
data:
  auto-extra-groups: (Base64解码后)system:bootstrappers:kubeadm:default-node-token
metadata:
  name: bootstrap-token-sm209r

  kubelet拿token访问apiserver时候，用到的token和用户组
```
  
  基于RBAC做权限控制
  
####  ClusterRole

查看clusterrole
kubectl get clusterrole | grep nodeclient
```
system:certificates.k8s.io:certificatesigningrequests:nodeclient
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
```

查看clusterole

`system:certificates.k8s.io:certificatesigningrequests:nodeclient` 定义如下内容

```
kind: ClusterRole
metadata:
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
rules:
- apiGroups:
  - vertificates.k8s.io
  resources:
  - certificatesigningrequests/nodeclient
  verbs:
  - create
```

kubelet用token访问apiserver时候，是可以进行操作 create certificatesigningrequests/nodeclient


####  ClusterRoleBinding

kubectl get clusterrolebinding | grep approve
```
kubeadm:node-autoapprove-bootstrap  创建证书，与CluserRole  xxx.nodeclient对应
kubeadm:node-autoapprove-certificate-rotation 更新证书，与ClusterRole XXX.selfnodeclient对应

```

 看看yaml文件什么样子
```
kind: ClusterRoleBinding
...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  # nodeclient对应的ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  # 上面那个token的secret里的组
  name: system:bootstrappers:kubeadm:default-node-token
```

  用token访问apiserver时，得到用户组system:bootstrappers:kubeadm:default-node-token，这个组在ClusterRoleBinding里绑定了ClusterRole名称system:certificates.k8s.io:certificatesigningrequests:nodeclient上，这个ClusterRole拥有创建csr请求的权限。至此整个流程通常。






#### 1 kubelet和kube-apiserver之前约定的token如何配置？

kubelet是通过 --bootstrap-kubeconfig=xxx/bootstrap-kubelet.conf配置文件写入token
apiserver，用secret。配置启动参数-enable-bootstrap-token-auth=true，自动创建Secret在kube-system。
           或者是token aulsfile（没听清）这个文件，把token内容写进去。
           
#### 2 启动时访问apiserver通过RBAC鉴权。
  secret、用户组，ClusterRole，ClusterRoleBinding


#### 3 controller-manager签发证书，启动参数中添加配置参数
--cluster-signing-cer-file=/etc/kubernetess/pki/ca.crt
--cluster-signing-key-file=/etc/kubenetes/pki/ca.key
以上3点同步具备后，集群会配置完成自动证书签发

#### 4 controller-manager启动参数配置签发证书的过期时间
--experimental-cluster-signing-duration


已提供了ca证书和私钥，配置集群添加节点自动签发证书
1，生成token文件，写入secret，配给apiserver，就是在apiserver配置参数 -enable-bootstrap-token-auth=true。kubelet节点上，把token文件放到默认的 --bootstrap-kubeconfig文件里写入token
2，controller-manager 把--cluster-signing-cer-file和--cluster-signing-key-file添加进去
3，创建clusterrole和clusterrolebinding，完成证书创建和自动续期两个



# 高可用
  master节点组件 liveness readness配置健康检查，多Pod多节点部署。多个节点部署etcd集群。apiserver多部署，前面部署LoadBalancer。controller-manager,Scheduler主备部署，抢夺分布式锁,主挂了备份还能抢到锁。
  node节点，因为是daemonset部署的pod，所以也可以配置健康检查
  docker、kubelet进程用Systemd来管理
  etcd周期备份，或者数据目录挂在云盘。云盘有快照或备份能力。
  
# 集群升级流程
  k8s升级重启不会影响业务Pod运行。升级过程还能继续提供服务。依次修改多个master节点manifest静态Pod，kubelet会watch这个目录，从而达到滚动升级。比如把apiserver镜像版本修改为高版本。
  node节点可以替换kubelet二进制包，kubelet关闭重启不会影响已经再节点上运行的pod。
  
  node节点机器重启
```
# 驱逐node上的pod

kubectl drain -h
kubectl drain <node_name>
kubectl drain --ignore-daemonsets=true 驱逐daemonset

kubectl uncordon <node_name>
```

kubeadm create token



作业
kubeadm部署一个集群
token文件生成，配置给apiserver.XXXX



