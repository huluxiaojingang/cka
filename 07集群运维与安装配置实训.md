# Instalation,Configuration &Validation 12%

# install master

# 集群部署

## Master节点
- etct
- kube-apiserver
- kube-controller-manager
- kube-scheduler
- ...

开始尝试virtualBox Ubuntu虚拟机部署，但是遇到一些奇怪的问题，最终放弃了采用阿里云两台ECS机器。

  
1、install docker
```
apt-get update
apt-get install -y docker.io
```

2、install kubelet kubeadm kubelet
```
关闭swap：vi /etc/fstab 注释掉/swap  重启系统

apt-get update && apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

3、kubeadm init
```
# kubeadm init 以Pod形式拉起 master 组件。
kubeadm init --pod-network-cidr=10.244.0.0/16 #pod分配的ip网段

# master 安装成功后需要，执行以下命令
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Static Pod
```
kubelet拉起 Static Pod 启动master组件。当kubelet启动时，kubelet --pod-manifest-path=<the directory>，自动创建目录下的所有pod
kubelet 进程 --config参数指定了启动参数，找到staticPodPath字段，
kubelet 会watch配置目录文件的变化，并作出响应。

如何启动Static Pod？ 找到kubelet --pod-manifest-path目录，把自己写的pod yaml文件丢在这个目录下，kubectl会自动创建Static Pod
```




## Node节点
- kubelet
- kube-proxy
- docker
- CNI 插件


同master节点一样安装 kubelet kubeadm kubelet

在master执行，查看kubeadm token
```
kubeadm token list
```

添加node节点命令
```
  kubeadm join 172.24.199.161:6443 --token p0kwhh.kg02s86ge9a0xxk2 --discovery-token-ca-cert-hash  sha256:82d04a08da055d5b8c4da0ebef112c01082081cd10d9ac4e377d187af109b5ec

```
master 节点执行查看node，可以发现已经添加了新的node，但是状态是NotReady
```
kubectl get nodes
```

安装CNI插件，这里用flannel，从官网找地址
```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml
```
master节点执行。会创建DaemonSet对象。意味着node也会自动安装。Daemonset安装成功后，集群正常部署完成。  可以执行  kubectl get nodes 显示节点状态是Ready。
kubeconfig相关概念clusters、users、contexts，多集群安全配置参考官网文档
https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

# 安全配置

## 集群组件如何交互

  apiserver与etcd之间会生成etcd相关证书，配置给apiserver用来访问etcd。
  
  kube-proxy、scheduler、kubelet、controller-manager通过kubeconfig文件，提供client认证凭据来访问apiserver.
  
  kubectl config view 查看kubeconfig
```
apiVersion: v1
clusters: # 集群信息，访问方式，CA等
- cluster:
    certificate-authority-data: REDACTED
    server: https://192.168.99.100:6443
  name: kubernetes
contexts: # cluster、user，
- context: # 集群和客户端认证方式映射关系
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users: # 客户端认证凭据，token，password，证书等
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```
## 生成kubeconfig文件
  kubectl config -h 获取帮助文档
  
  修改cluster
```
kubectl config set-cluster kubernetes
      --certificate-authority=/path/ti/ca
      --embed-certs=true
      --server=${KUBE_APISERVER}
      --kubeconfig=/kubeconfig/filename
```

  修改user
```
kubectl config set-credentials testuser
      --client-certificate=/path/to/cert
      --client-key=/path/to/private_key
      --embed-certs=true
      --kubeconfig/kubeconfig/filename
``` 

  修改context
```
kubectl config set-context default
  --cluster=
  --user=
  --namespace=
  --kubeconfig=test.kubeconfig
```
  
  设置默认context
```
kubectl config use-context default --kubeconfig=/path/to/filename
```
  生成kubeconfig文件
```
#!/bin/bash

export KUBE_APISERVER="https://172.24.92.189:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.crt \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=test.kubeconfig
  
# --kubeconfig=test.kubeconfig 已经存在则新增，没有则创建
# --embed-certs=true 会把CA内容写进去，不加的话certificate-authority存的是证书路径
  
kubectl config set-credentials testuser \
  --client-certificate=/etc/kubernetes/pki/apiserver.crt \
  --client-key=/etc/kubernetes/pki/apiserver.key \
  --embed-certs=true \
  --kubeconfig=test.kubeconfig
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=testuser \
  --kubeconfig=test.kubeconfig
  
kubectl config use-context default --kubeconfig=test.kubeconfig

# 使用test.kubeconfig访问apiserver
kubectl get pods --kubeconfig=./test.kubeconfig

```
  kubeconfig文件是kube-proxy、scheduler、kubelet、controller-manager组件访问kube-apiserver用的


## TLS bootstrap与节点证书签发
  一个新的node节点加入k8s集群时，给节点签发客户端证书，节点名作为kubelet访问apiserver的用户名。不同的节点用户名不一样。当集群规模越来越大，证书签发任务太繁重。所以需要一种自动证书签发。
  
####   自动证书签发流程
 - kubelet启动时使用低权限token向kube-apiserver发送csr请求。kubeadm join --token 里的token。rbac配置权限
 - controller-mmanager会自动把证书签发下来给kube-apiserver。kubelet等待apiserver，对应的CRS资源对象在status字段里会写入证书。
 - kubelet拿到证书后写到节点证书路径下。
 - kubelet用新的证书访问apiserver

#### RBAC允许的CRS请求类型

   kubelet第一次启动后发送签发证书请求给apiserver，鉴权会绑定ClusterRole，有以下三类：
- nodeclient：签发证书
- selfnodeclient：更新证书
- selfnodeserver：更新kubelet server 证书

kubelet启动参数
```
# 这个配置里有token，就是kubeadm join --token对应的token
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf

--kubeconfig=/etc/kubernetes/kubelet.conf
```

apiserver启动参数
```
--enable-bootstrap-token-auth=true ,在kube-system下创建Secret，bootstrap-token=xxx用于鉴权。

```


ps -ef | grep kube-apiserver 查看启动参数中有 -enable-bootstrap-token-auth=true。配置这个参数会自动在kube-system下创建bootstrap-token-xxx的secret，名字为bootstrap-token-XXX，查看这个secret的yaml文件。查看data.auth-extra-groups，base64解码后显示system:bootstrappers:kubeadm:default-node-token。

#### Secret

kubectl get secret bootstrap-token-sm209r -n kube-system -o yaml

```
...
kind: Secret
data:
  auto-extra-groups: (Base64解码后)system:bootstrappers:kubeadm:default-node-token
metadata:
  name: bootstrap-token-sm209r

```


```
kubectl get clusterrole | grep nodeclient
system:certificates.k8s.io:certificatesigningrequests:nodeclient
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
```

#### ClusterRole  system:certificates.k8s.io:certificatesigningrequests:nodeclient内容
```
kind: ClusterRole
metadata:
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
rules:
- apiGroups:
  - vertificates.k8s.io
  resources:
  - certificatesigningrequests/nodeclient
  verbs:
  - create
...

```


kubelet拿到上面的token访问apiserver时，clusterrole配置了verbs:create，resources：certificatesigningrequests/nodeclient，可以调用apiserver

再看下clusterrolebinding   kubeadm:node-autoapprove-bootstrap
kubectl get clusterrolebinding | grep bootstrap


#### ClusterRoleBinding
kubectl get clusterrolebinding  kubeadm:node-autoapprove-bootstrap -o yaml
```
kind: ClusterRoleBinding
...
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers:kubeadm:default-node-token

```

#### 1kubelet和kube-apiserver之前约定的token如何配置？

kubelet是通过 --bootstrap-kubeconfig=xxx/bootstrap-kubelet.conf配置文件写入token
apiserver，用secret。配置启动参数-enable-bootstrap-token-auth=true，自动创建Secret在kube-system下，完成鉴权。
           或者autofile（没听清）这个文件，把token内容写进去。
           
#### 2启动时访问apiserver通过RBAC鉴权，关注ClusterRole和ClusterRolebinding


#### 3controller-manager签发证书，启动参数中添加配置参数
--cluster-signing-cer-file=/etc/kubernetess/pki/ca.crt
--cluster-signing-key-file=/etc/kubenetes/pki/ca.key
以上3点同步具备后，集群会配置完成自动证书签发


已提供了ca证书和私钥，配置集群添加节点自动签发证书
1，生成token文件，写入secret，配给apiserver，就是在apiserver配置参数 -enable-bootstrap-token-auth=true
2，kubelet节点上，把token文件放到默认的 --bootstrap-kubeconfig文件里写入token
3，controller-manager 把--cluster-signing-cer-file和--cluster-signing-key-file添加进去
4，创建clusterrole和clusterrolebinding，完成证书创建和自动续期两个














